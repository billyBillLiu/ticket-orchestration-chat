# app/services/planner_service.py
from __future__ import annotations
import json
from typing import Dict
from app.services.catalog_service import slice_catalog_for_prompt
from app.models.ticket_agent import TicketPlan
from app.services.llm_service import chat  # <-- existing Ollama wrapper
from datetime import datetime

# System prompt keeps the model focused on producing a JSON plan only.
SYSTEM_PLAN = """
You are a Service Ticket Planner.
Return ONLY JSON. No prose.

Using the allowed catalog (departments, categories, ticket types), propose 1-3 ticket items
that satisfy the user's request.

CRITICAL RULES:
- Use only the provided categories and ticket types from the catalog.
- Choose concise, action-oriented titles.
- If the user doesn't provide enough information to fill a field, leave the form completely empty - if you are not confident that you can fill a field with the information provided, leave it empty.
- Do NOT use placeholder text like "your_email@example.com" or "[Investor Name]".
- Do NOT use example values like "user@example.com".
- Do NOT invent fields or values outside the catalog.
- Do NOT invent ticket types - use ONLY the exact ticket_type names from the catalog.
- The email field will be automatically filled by the system - do NOT prefill it unless a specific email is provided in the user's text.
- The summary field will be automatically generated by the system after all other fields are filled - do NOT prefill it.
- For date fields: Use the CURRENT DATE context to convert relative dates (e.g., "tomorrow" ‚Üí actual date).
- Return ONLY valid JSON - NO comments, NO trailing commas, NO extra text.

This is the only output format you are allowed to return:
{
  "items": [
    {
      "service_area": "<area key>",
      "category": "<category name>",
      "ticket_type": "<ticket type>",
      "title": "<short actionable title>",
      "description": "<short context>",
      "form": {},      // key:value map for fields
      "labels": []
    }
  ],
  "meta": { "request_text": "<echo of user text>" }
}

EXAMPLES:
- User says "I need a final loan tape for AAA" ‚Üí ticket_type: "Loan Tape", vendor_name: "AAA Final Loan Tape", type_of_rerun: "final"
- User says "investor report rerun from july 18 to tomorrow" ‚Üí ticket_type: "Investor Reports Manual Rerun", start_date: "2024-07-18", end_date: "2024-01-16" (using current date context)
- User says "high urgency datadog monitoring" ‚Üí urgency: "high"
"""



async def plan_from_text_async(user_text: str, user_email: str = None) -> TicketPlan:
    """
    Async version of plan_from_text that works within FastAPI's async context.
    """
    print(f"üîç PLANNER: Starting async plan_from_text for: '{user_text}'")
    
    cat_slice = slice_catalog_for_prompt(user_text)
    print(f"üìã PLANNER: Catalog slice has {len(cat_slice.get('service_areas', []))} service areas")

    # Add current datetime context
    current_datetime = datetime.now()
    datetime_context = f"\n\nCURRENT DATE AS REFERENCE: {current_datetime.strftime('%Y-%m-%d')}"

    # Add user email context if available
    user_context = ""
    if user_email:
        user_context = f"\n\nUSER EMAIL: {user_email}\nThe email field should be automatically filled with this email address."

    messages = [
        {"role": "system", "content": SYSTEM_PLAN + datetime_context + user_context},
        # Put the catalog slice as JSON in the assistant "context" turn,
        # so the model can see the allowed categories/ticket types/fields.
        {"role": "assistant", "content": json.dumps({"catalog": cat_slice})},
        {"role": "user", "content": user_text}
    ]

    print(f"ü§ñ PLANNER: Calling LLM with {len(messages)} messages")
    try:
        # Use the LLM service directly instead of the chat function
        from app.services.llm_service import llm_service, Message as LLMMessage
        
        # Convert messages to Message objects
        message_objects = [LLMMessage(**msg) for msg in messages]
        
        # Call LLM directly - don't use context manager to keep client open
        response = await llm_service.generate_non_streaming_response(
            messages=message_objects,
            model="llama3:8b",  # Use the actual model name
            temperature=0.2,
            max_tokens=4096
        )
        
        raw = response.content
        print(f"üì• PLANNER: LLM returned raw response: '{raw[:100]}...'")
        
        # Clean up the response - remove markdown formatting and extra text
        print(f"üîß PLANNER: Cleaning up LLM response...")
        
        # Look for JSON code blocks
        if "```json" in raw:
            # Extract JSON from markdown code block
            start = raw.find("```json") + 7
            end = raw.find("```", start)
            if end != -1:
                raw = raw[start:end].strip()
                print(f"üîß PLANNER: Extracted JSON from code block")
        elif "```" in raw:
            # Extract JSON from regular code block
            start = raw.find("```") + 3
            end = raw.find("```", start)
            if end != -1:
                raw = raw[start:end].strip()
                print(f"üîß PLANNER: Extracted JSON from regular code block")
        
        # Try to find JSON object boundaries if still not clean
        if not raw.strip().startswith("{"):
            # Look for the first { and last }
            start_brace = raw.find("{")
            end_brace = raw.rfind("}")
            if start_brace != -1 and end_brace != -1 and end_brace > start_brace:
                raw = raw[start_brace:end_brace + 1]
                print(f"üîß PLANNER: Extracted JSON object from text")
        
        print(f"üîß PLANNER: Cleaned content: {raw[:100]}...")
        
        # Try to parse the JSON
        try:
            data = json.loads(raw)
            print(f"‚úÖ PLANNER: Successfully parsed JSON, creating TicketPlan")
            return TicketPlan(**data)
        except json.JSONDecodeError as e:
            print(f"‚ùå PLANNER: JSON parsing failed: {e}")
            print(f"Raw content: {raw}")
            raise
    except Exception as e:
        # Re-raise the exception if LLM fails
        print(f"‚ùå PLANNER: LLM failed: {e}")
        raise

def plan_from_text(user_text: str, user_email: str = None) -> TicketPlan:
    """
    1) Slice the catalog based on the user message (reduce noise).
    2) Ask the LLM to produce a small plan (1-3 items) using that slice.
    3) Parse as JSON and validate against TicketPlan Pydantic model.
    """
    print(f"üîç PLANNER: Starting plan_from_text for: '{user_text}'")
    
    cat_slice = slice_catalog_for_prompt(user_text)
    print(f"üìã PLANNER: Catalog slice has {len(cat_slice.get('service_areas', []))} service areas")

    # Add current datetime context
    current_datetime = datetime.now()
    datetime_context = f"\n\nCURRENT DATE AS REFERENCE: {current_datetime.strftime('%Y-%m-%d')}"

    # Add user email context if available
    user_context = ""
    if user_email:
        user_context = f"\n\nUSER EMAIL: {user_email}\nThe email field should be automatically filled with this email address."

    messages = [
        {"role": "system", "content": SYSTEM_PLAN + datetime_context + user_context},
        # Put the catalog slice as JSON in the assistant "context" turn,
        # so the model can see the allowed categories/ticket types/fields.
        {"role": "assistant", "content": json.dumps({"catalog": cat_slice})},
        {"role": "user", "content": user_text}
    ]

    print(f"ü§ñ PLANNER: Calling LLM with {len(messages)} messages")
    try:
        # Important: enforce JSON-only output from the model
        raw = chat(
            messages,
            model="llama3.1:8b",   # substitute your local model
            format="json",
            options={"temperature": 0.2, "top_p": 0.9, "num_ctx": 4096},
        )

        print(f"üì• PLANNER: LLM returned raw response: '{raw[:100]}...'")
        
        data = json.loads(raw)
        print(f"‚úÖ PLANNER: Successfully parsed JSON, creating TicketPlan")
        return TicketPlan(**data)
    except Exception as e:
        # Re-raise the exception if LLM fails
        print(f"‚ùå PLANNER: LLM failed: {e}")
        raise
